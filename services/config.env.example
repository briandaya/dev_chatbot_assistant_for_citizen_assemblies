# Configuration for the default LLM Ollama model
MODELS_SERVER = 'https://example/ollama'
MODELS_LOCAL = 'http://ollama:11434'
LLM_BASE_URL = 'https://example/ollama'
LLM_MODEL_NAME = 'llama3'
LLM_KEEP_ALIVE = 0
LLM_REQUEST_TIMEOUT = 60
LLM_TEMPERATURE = 0.5 # 0.75

# Configuration for text embedding inference
EMBED_SERVER = 'https://example2/hg-embeddings'
EMBED_LOCAL = 'http://ollama:11434'
EMBED_BASE_URL = "https://example2/hg-embeddings"
EMBED_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
EMBED_TIMEOUT = 60
EMBED_BATCH_SIZE = 10

# Configuration for chunks (only for non-markdown documents)
CHUNK_SIZE = 512
CHUNK_OVERLAP = 20

# Configuration for Weaviate (for access inside docker compose. For access from outside but local, redirect in etc/hosts weaviate to localhost)
WEAVIATE_URL = 'http://weviate:8080/'
WEAVIATE_HOST = 'weaviate'
WEAVIATE_PORT = '8080'
WEAVIATE_GRPC_PORT = '50051'

# Collection name in Weaviate
INDEX_NAME = 'Documents_acc01'
INDEX_NAME_TEMP = 'Temporal_web_search'

# Configuration for chunks (only for non-markdown documents)
CHUNK_SIZE = 512
CHUNK_OVERLAP = 20

# Configuration for Orchestator
ORCHESTATOR_URL = 'http://orchestator:8010'
AVAILABLE_MODELS = '[{"MODEL_LABEL": "Cloud-ollama-llama3", "MODEL_NAME": "llama3"}]'